

Este es el resultado del entrenamiento sobre una placa grafica  GTX 1050 Ti (4GB VRAM, 768 núcleos CUDA)
Modelo: microsoft/phi-2 (1.5B parámetros)


'grad_norm': 0.8693416714668274, 'learning_rate': 2.4e-05, 'epoch': 6.3}
{'loss': 2.025, 'grad_norm': 1.435265064239502, 'learning_rate': 4.9e-05, 'epoch': 12.6}
{'loss': 1.2582, 'grad_norm': 1.3921124935150146, 'learning_rate': 7.4e-05, 'epoch': 18.91}
{'loss': 0.8179, 'grad_norm': 2.988405466079712, 'learning_rate': 9.900000000000001e-05, 'epoch': 25.0}
{'loss': 0.4366, 'grad_norm': 1.7632297277450562, 'learning_rate': 9.982464296247522e-05, 'epoch': 31.3}
{'loss': 0.2138, 'grad_norm': 1.4847832918167114, 'learning_rate': 9.927039492417452e-05, 'epoch': 37.6}
{'loss': 0.1361, 'grad_norm': 1.5613294839859009, 'learning_rate': 9.834116943022298e-05, 'epoch': 43.91}
{'loss': 0.1035, 'grad_norm': 2.1133832931518555, 'learning_rate': 9.704403844771128e-05, 'epoch': 50.0}
{'loss': 0.0929, 'grad_norm': 0.8091534376144409, 'learning_rate': 9.538887392664544e-05, 'epoch': 56.3}
{'loss': 0.0816, 'grad_norm': 0.8248382210731506, 'learning_rate': 9.338827266844644e-05, 'epoch': 62.6}
{'loss': 0.0791, 'grad_norm': 0.8037989139556885, 'learning_rate': 9.105746045668521e-05, 'epoch': 68.91}
{'loss': 0.0733, 'grad_norm': 1.202606439590454, 'learning_rate': 8.841417617967618e-05, 'epoch': 75.0}
{'loss': 0.0692, 'grad_norm': 0.6855400204658508, 'learning_rate': 8.547853682682604e-05, 'epoch': 81.3}
{'loss': 0.0688, 'grad_norm': 0.5450165867805481, 'learning_rate': 8.227288438619754e-05, 'epoch': 87.6}
{'loss': 0.065, 'grad_norm': 0.5289790034294128, 'learning_rate': 7.882161580848967e-05, 'epoch': 93.91}
{'loss': 0.0644, 'grad_norm': 0.7438283562660217, 'learning_rate': 7.515099733151177e-05, 'epoch': 100.0}
{'loss': 0.066, 'grad_norm': 0.5085850954055786, 'learning_rate': 7.128896457825364e-05, 'epoch': 106.3}
{'loss': 0.0609, 'grad_norm': 0.43232840299606323, 'learning_rate': 6.726490994992674e-05, 'epoch': 112.6}
{'loss': 0.0619, 'grad_norm': 0.4322701692581177, 'learning_rate': 6.310945893204324e-05, 'epoch': 118.91}
{'loss': 0.0605, 'grad_norm': 0.8138301968574524, 'learning_rate': 5.885423701597917e-05, 'epoch': 125.0}
{'loss': 0.0606, 'grad_norm': 0.4907078444957733, 'learning_rate': 5.453162900988902e-05, 'epoch': 131.3}
{'loss': 0.0611, 'grad_norm': 0.39632293581962585, 'learning_rate': 5.017453257076119e-05, 'epoch': 137.6}
{'loss': 0.0608, 'grad_norm': 0.45842498540878296, 'learning_rate': 4.5816107833384234e-05, 'epoch': 143.91}
{'loss': 0.0588, 'grad_norm': 0.6726220846176147, 'learning_rate': 4.1489525041698387e-05, 'epoch': 150.0}
{'loss': 0.0583, 'grad_norm': 0.3945554494857788, 'learning_rate': 3.7227712103210486e-05, 'epoch': 156.3}
{'loss': 0.0588, 'grad_norm': 0.43602991104125977, 'learning_rate': 3.3063103987735433e-05, 'epoch': 162.6}
{'loss': 0.0583, 'grad_norm': 0.5655059814453125, 'learning_rate': 2.9027395877691144e-05, 'epoch': 168.91}
{'loss': 0.0589, 'grad_norm': 0.5826209187507629, 'learning_rate': 2.5151301948622237e-05, 'epoch': 175.0}
{'loss': 0.0576, 'grad_norm': 0.36816883087158203, 'learning_rate': 2.1464321615778422e-05, 'epoch': 181.3}
{'loss': 0.0568, 'grad_norm': 0.4754781723022461, 'learning_rate': 1.7994515025752217e-05, 'epoch': 187.6}
{'loss': 0.0578, 'grad_norm': 0.3511523902416229, 'learning_rate': 1.4768289501820265e-05, 'epoch': 193.91}
{'loss': 0.056, 'grad_norm': 0.6463200449943542, 'learning_rate': 1.1810198568267905e-05, 'epoch': 200.0}
{'loss': 0.0577, 'grad_norm': 0.3609299659729004, 'learning_rate': 9.142755083243576e-06, 'epoch': 206.3}
{'loss': 0.0571, 'grad_norm': 0.39858514070510864, 'learning_rate': 6.786259902314768e-06, 'epoch': 212.6}
{'loss': 0.0567, 'grad_norm': 0.4042302370071411, 'learning_rate': 4.758647376699032e-06, 'epoch': 218.91}
{'loss': 0.0571, 'grad_norm': 0.5709611773490906, 'learning_rate': 3.0753488620222037e-06, 'epoch': 225.0}
{'loss': 0.0562, 'grad_norm': 0.37878236174583435, 'learning_rate': 1.7491752763844293e-06, 'epoch': 231.3}                                                                                                                                                                              
{'loss': 0.0564, 'grad_norm': 0.36430636048316956, 'learning_rate': 7.90219601537906e-07, 'epoch': 237.6}                                                                                                                                                                                
{'loss': 0.0556, 'grad_norm': 0.3741146922111511, 'learning_rate': 2.057800692014833e-07, 'epoch': 243.91}                                                                                                                                                                               
{'loss': 0.057, 'grad_norm': 0.7443798184394836, 'learning_rate': 3.0461711048035415e-10, 'epoch': 250.0}                                                                                                                                                                                
{'train_runtime': 117205.4494, 'train_samples_per_second': 0.137, 'train_steps_per_second': 0.009, 'train_loss': 0.24024286901950836, 'epoch': 250.0}                                                                                                                                    
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [32:33:25<00:00, 117.21s/it] 

✅ ¡Entrenamiento completado en 1953.67 minutos!
***** train metrics *****
  epoch                    =             250.0
  total_flos               =       100852232GF
  train_loss               =            0.2402
  train_runtime            = 1 day, 8:33:25.44
  train_samples_per_second =             0.137
  train_steps_per_second   =             0.009

12. Guardando modelo final...
Adaptador LoRA guardado en: models/phi2-matematicas-tutor
Tokenizador guardado en: models/phi2-matematicas-tutor

✅ ¡Proceso completo! Modelo guardado en models/phi2-matematicas-tutor

Puedes usar el modelo fine-tuned para conceptos matemáticos ejecutando:
  python evaluate.py

=== USO FINAL DE MEMORIA ===
GPU: NVIDIA GeForce GTX 1050 Ti
Memoria asignada: 2326.09 MB
Memoria reservada: 3228.00 MB
Memoria máxima asignada: 3087.42 MB
